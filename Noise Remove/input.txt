
CHAPTER 1. INTRODUCTION
so has the complexity of the tasks that they can solve. Goodfellow et al. (2014d)
showed that neural networks could learn to output an entire sequence of characters
transcribed from an image, rather than just identifying a single object. Previously,
it was widely believed that this kind of learning required labeling of the individual
elements of the sequence (Gülçehre and Bengio, 2013). Recurrent neural networks,
such as the LSTM sequence model mentioned above, are now used to model
relationships between sequences and other sequences rather than just ﬁxed inputs.
This sequence-to-sequence learning seems to be on the cusp of revolutionizing
another application: machine translation (Sutskever et al., 2014; Bahdanau et al.,
2015).
This trend of increasing complexity has been pushed to its logical conclusion
with the introduction of neural Turing machines (Graves et al., 2014) that learn
to read from memory cells and write arbitrary content to memory cells. Such
neural networks can learn simple programs from examples of desired behavior. For
example, they can learn to sort lists of numbers given examples of scrambled and
sorted sequences. This self-programming technology is in its infancy, but in the
future it could in principle be applied to nearly any task.
Another crowning achievement of deep learning is its extension to the domain of
reinforcement learning
. In the context of reinforcement learning, an autonomous
agent must learn to perform a task by trial and error, without any guidance from
the human operator. DeepMind demonstrated that a reinforcement learning system
based on deep learning is capable of learning to play Atari video games, reaching
human-level performance on many tasks (Mnih et al., 2015). Deep learning has
also signiﬁcantly improved the performance of reinforcement learning for robotics
(Finn et al., 2015).
Many of these applications of deep learning are highly proﬁtable. Deep learning
is now used by many top technology companies, including Google, Microsoft,
Facebook, IBM, Baidu, Apple, Adobe, Netﬂix, NVIDIA, and NEC.
Advances in deep learning have also depended heavily on advances in software
infrastructure. Software libraries such as Theano (Bergstra et al., 2010; Bastien
et al., 2012), PyLearn2 (Goodfellow et al., 2013c), Torch (Collobert et al., 2011b),
DistBelief (Dean et al., 2012), Caﬀe (Jia, 2013), MXNet (Chen et al., 2015), and
TensorFlow (Abadi et al., 2015) have all supported important research projects or
commercial products.
Deep learning has also made contributions to other sciences. Modern convolu-
tional networks for object recognition provide a model of visual processing that
neuroscientists can study (DiCarlo, 2013). Deep learning also provides useful tools
for processing massive amounts of data and making useful predictions in scientiﬁc
25
CHAPTER 1. INTRODUCTION
ﬁelds. It has been successfully used to predict how molecules will interact in order
to help pharmaceutical companies design new drugs (Dahl et al., 2014), to search
for subatomic particles (Baldi et al., 2014), and to automatically parse microscope
images used to construct a 3-D map of the human brain (Knowles-Barley et al.,
2014). We expect deep learning to appear in more and more scientiﬁc ﬁelds in the
future.
In summary, deep learning is an approach to machine learning that has drawn
heavily on our knowledge of the human brain, statistics and applied math as it
developed over the past several decades. In recent years, deep learning has seen
tremendous growth in its popularity and usefulness, largely as the result of more
powerful computers, larger datasets and techniques to train deeper networks. The
years ahead are full of challenges and opportunities to improve deep learning even
further and to bring it to new frontiers.
26

